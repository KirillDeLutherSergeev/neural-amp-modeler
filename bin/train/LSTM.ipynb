{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC3XkMetGWtK"
      },
      "source": [
        "# Neural Amp Modeler (Trainer)\n",
        "This notebook allows you to train a neural amp model based on a pair of input/output WAV files that you have of the amp you want to model.\n",
        "\n",
        "**To use this notebook**:\n",
        "Go to [colab.research.google.com](https://colab.research.google.com/), select the \"GitHub\" tab, and select this notebook. Or, if you've cloned the repo, you can upload it from your computer.\n",
        "\n",
        "ðŸ”¶**Before you run**ðŸ”¶\n",
        "\n",
        "Make sure to get a GPU! (Runtime->Change runtime type->Select \"GPU\" from the \"Hardware accelerator dropdown menu)\n",
        "\n",
        "âš **Warning**âš \n",
        "\n",
        "Google Colab GPU instances only last for 12 hours.\n",
        "Plan your training accordingly!\n",
        "\n",
        "## Steps:\n",
        "0. Install everything\n",
        "1. Upload audio files\n",
        "2. Settings\n",
        "3. Run!\n",
        "4. Check\n",
        "5. Export\n",
        "6. Download your files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qxdBa6G9dUfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g_4GtFuGlO8"
      },
      "source": [
        "## Step 0: Install\n",
        "Install `nam` and the other Python packages it depends on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYQIpWr5EYRb"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/KirillDeLutherSergeev/neural-amp-modeler.git@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6GUkLz3EayL"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from typing import Optional, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from google.colab import files\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from nam.data import Split, init_dataset\n",
        "from nam.models import Model\n",
        "\n",
        "import wavio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CQleTk7GJV8"
      },
      "source": [
        "## Step 1: Upload audio files\n",
        "Upload the input (DI) and output (amped) files you want to use.\n",
        "\n",
        "You'll need two pairs of files (4 in total):\n",
        "* A training pair (`x_train.wav`, `y_train.wav`) for the model to fit to.\n",
        "* A validation pair, (`x_validation.wav`, `y_validation.wav`) to check the model's performance on a new signal.\n",
        "\n",
        "The **default names** for the training data are `x_train.wav` (DI input) and `y_train.wav` (amped output), and for the validation set, `x_validation.wav` and `y_validation.wav`. \n",
        "\n",
        "If you files are named differently, don't worry--you can modify the names in the data config below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_filL-5F8HR"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5fN10s3GwVz"
      },
      "source": [
        "## Step 2: Settings\n",
        "The defaults are what I tend to start with and should usually work well (except the file names--see above), but if you'd like, you can make changes.\n",
        "\n",
        "ðŸ”¶**Important**ðŸ”¶\n",
        "\n",
        "If there is a **delay** between your input and output (e.g. if you reamped and your interface has latency that your DAW doesn't factor out), then change that in the `data_config` dict below to the delay, in samples. For example, if the output lags the input by 505 samples, then change it to `\"delay\": 505`. If there's too much delay (e.g. more than 10 samples), then the model may not learn. The closer you get this, the better the results will be, but don't over-compensate or else the model would have to predict the future!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6gl6RoNJ_6I"
      },
      "outputs": [],
      "source": [
        "data_config = {\n",
        "    \"train\": {\n",
        "        \"x_path\": \"drive/MyDrive/ML/5153B/x_train.wav\",\n",
        "        \"y_path\": \"drive/MyDrive/ML/5153B/y_train.wav\",\n",
        "        \"ny\": 32768\n",
        "    },\n",
        "    \"validation\": {\n",
        "        \"x_path\": \"drive/MyDrive/ML/5153B/x_validation.wav\",\n",
        "        \"y_path\": \"drive/MyDrive/ML/5153B/y_validation.wav\",\n",
        "        \"ny\": None\n",
        "    },\n",
        "    \"common\": {\n",
        "        \"delay\": 0\n",
        "    }\n",
        "}\n",
        "model_config = {\n",
        "    \"net\": {\n",
        "        \"name\": \"LSTM\",\n",
        "        \"config\": {\n",
        "            \"num_layers\": 3,\n",
        "            \"hidden_size\": 48,\n",
        "            \"train_burn_in\": 4096,\n",
        "            \"train_truncate\": 512            \n",
        "        }\n",
        "    },\n",
        "    \"loss\": {\n",
        "        \"val_loss\": \"mse\",\n",
        "        \"mask_first\": 4096,    \n",
        "        \"pre_emph_weight\": 1.0,\n",
        "        \"pre_emph_coef\": 0.85\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"lr\": 0.01\n",
        "    },\n",
        "    \"lr_scheduler\": {\n",
        "        \"class\": \"ExponentialLR\",\n",
        "        \"kwargs\": {\n",
        "            \"gamma\": 0.995\n",
        "        }\n",
        "    }\n",
        "}\n",
        "learning_config = {\n",
        "    \"train_dataloader\": {\n",
        "        \"batch_size\": 32,\n",
        "        \"shuffle\": True,\n",
        "        \"pin_memory\": True,\n",
        "        \"drop_last\": True,\n",
        "        \"num_workers\": 0\n",
        "    },\n",
        "    \"val_dataloader\": {},\n",
        "    \"trainer\": {\n",
        "        \"gpus\": 1,\n",
        "        \"max_epochs\": 100\n",
        "    },\n",
        "    \"trainer_fit_kwargs\": {}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNga-MNTMQAa"
      },
      "source": [
        "## Step 3: Run!\n",
        "Let's rock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAChcygdMTF4"
      },
      "outputs": [],
      "source": [
        "model = Model.init_from_config(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ1jCOh7i7Ct"
      },
      "outputs": [],
      "source": [
        "data_config[\"common\"][\"nx\"] = model.net.receptive_field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV4gLukTMjdD"
      },
      "outputs": [],
      "source": [
        "dataset_train = init_dataset(data_config, Split.TRAIN)\n",
        "dataset_validation = init_dataset(data_config, Split.VALIDATION)\n",
        "train_dataloader = DataLoader(dataset_train, **learning_config[\"train_dataloader\"])\n",
        "val_dataloader = DataLoader(dataset_validation, **learning_config[\"val_dataloader\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyhMf0ZyM4kt"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    callbacks=[\n",
        "        pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
        "            filename=\"{epoch:04d}_{step}_{ESR:.3e}_{MSE:.3e}\",\n",
        "            save_top_k=3,\n",
        "            monitor=\"val_loss\",\n",
        "            every_n_epochs=1,\n",
        "        ),\n",
        "        pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
        "            filename=\"checkpoint_last_{epoch:04d}_{step}\", every_n_epochs=1\n",
        "        ),\n",
        "    ],\n",
        "    **learning_config[\"trainer\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8WLIx33M7c6"
      },
      "outputs": [],
      "source": [
        "# Here we go!\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    **learning_config.get(\"trainer_fit_kwargs\", {}),\n",
        ")\n",
        "# Monitor the progress in lightning_logs/version_0/checkpoints.\n",
        "# Tthe ESR may plateau at 1 for about 20 iterations, but if it hasn't started decreasing\n",
        "# By then, then something might be wrong e.g. mismatched data, or an incorrect delay \n",
        "# (see above).\n",
        "#\n",
        "# Many models can get a good result (rule of thumb: look for ESR<0.01) in about 15 \n",
        "# minutes of training, but if you're more patient, it'll probably keep getting better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzGltwwJNAkI"
      },
      "outputs": [],
      "source": [
        "# Go to best checkpoint\n",
        "best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
        "if best_checkpoint != \"\":\n",
        "    model = Model.load_from_checkpoint(\n",
        "        trainer.checkpoint_callback.best_model_path,\n",
        "        **Model.parse_config(model_config),\n",
        "    )\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvuJEYxJNGn7"
      },
      "source": [
        "# Step 4: Check\n",
        "Let's look at how well our model matches the real thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0UeoIbaNMxF"
      },
      "outputs": [],
      "source": [
        "from nam.models.conv_net import REQUIRED_RATE\n",
        "def _rms(x: Union[np.ndarray, torch.Tensor]) -> float:\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return np.sqrt(np.mean(np.square(x)))\n",
        "    elif isinstance(x, torch.Tensor):\n",
        "        return torch.sqrt(torch.mean(torch.square(x))).item()\n",
        "    else:\n",
        "        raise TypeError(type(x))\n",
        "\n",
        "def plot(\n",
        "    model,\n",
        "    ds,\n",
        "    savefig=None,\n",
        "    show=True,\n",
        "    window_start: Optional[int] = None,\n",
        "    window_end: Optional[int] = None,\n",
        "):\n",
        "    with torch.no_grad():\n",
        "        tx = len(ds.x) / REQUIRED_RATE\n",
        "        print(f\"Run (t={tx})\")\n",
        "        t0 = time()\n",
        "        output = model(ds.x).flatten().cpu().numpy()\n",
        "        t1 = time()\n",
        "        print(f\"Took {t1 - t0} ({tx / (t1 - t0):.2f}x)\")\n",
        "        wavio.write(\n",
        "        str(\"test_output.wav\"),\n",
        "        output,\n",
        "        REQUIRED_RATE,\n",
        "        sampwidth=4,\n",
        "        )\n",
        "        \n",
        "    plt.figure(figsize=(16, 5))\n",
        "    # plt.plot(ds.x[window_start:window_end], label=\"Input\")\n",
        "    plt.plot(output[window_start:window_end], label=\"Prediction\")\n",
        "    plt.plot(ds.y[window_start:window_end], linestyle=\"--\", label=\"Target\")\n",
        "    # plt.plot(\n",
        "    #     ds.y[window_start:window_end] - output[window_start:window_end], label=\"Error\"\n",
        "    # )\n",
        "    plt.title(f\"NRMSE={100.0 * _rms(torch.Tensor(output) - ds.y) / _rms(ds.y):2.1f}%\")\n",
        "    plt.legend()\n",
        "    if savefig is not None:\n",
        "        plt.savefig(savefig)\n",
        "    if show:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_NsBdp5NQMC"
      },
      "outputs": [],
      "source": [
        "plot(\n",
        "    model,\n",
        "    dataset_validation,\n",
        "    window_start=100_000,  # Start of the plotting window, in samples\n",
        "    window_end=101_000,  # End of the plotting window, in samples\n",
        ")\n",
        "# Don't worry if the LSTM runs slower than 1x. Python's interpreter is slow, and the C++\n",
        "# version is a lot faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R__jJFwgNkAl"
      },
      "source": [
        "## Step 5: Export your model\n",
        "Now we'll use NAM's exporting utility to convert the model from its PyTorch representation to something that you can put into the plugin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQDcgoi_NrsW"
      },
      "outputs": [],
      "source": [
        "# This isn't used right now, but I might use it in the future :)\n",
        "# model.export(\".\")\n",
        "\n",
        "model.net.export_cpp_header(\"HardCodedModel.h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823KJ_L0Rchp"
      },
      "source": [
        "## Step 6: Download your artifacts\n",
        "We're done! \n",
        "Go to the file browser on the left panel â¬… and collect your artifacts!\n",
        "\n",
        "Be sure to download the lightning model artifacts (in case you want to continue training later) and your exported model (so that you can put it into a plugin)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.net.export(\".\")"
      ],
      "metadata": {
        "id": "5hN4EdBY5YnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate(\n",
        "    model,\n",
        "    ds\n",
        "):\n",
        "    with torch.no_grad():\n",
        "        tx = len(ds.x) / REQUIRED_RATE\n",
        "        print(f\"Run (t={tx})\")\n",
        "        t0 = time()\n",
        "        output = model(ds.x).flatten().cpu().numpy()\n",
        "        t1 = time()\n",
        "        print(f\"Took {t1 - t0} ({tx / (t1 - t0):.2f}x)\")\n",
        "        wavio.write(\n",
        "        str(\"test_output.wav\"),\n",
        "        output,\n",
        "        REQUIRED_RATE,\n",
        "        sampwidth=4,\n",
        "        )"
      ],
      "metadata": {
        "id": "WbEzqVOj8R0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate(model,dataset_validation)"
      ],
      "metadata": {
        "id": "4Y0I-UsQ-kuC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('nam-dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "806b1ffcfc1136d2cb83a025e53d737ae1674e4fb75a9a1fd5fb23a0ac3e6e42"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}